#pragma once
#pragma OPENCL EXTENSION cl_khr_subgroups : enable

#define VECT4 1
#define SHARED_BANKS_32 1 
#define SUBGROUP_32 1

#if VECT2
    constexpr uint SIMD = 2;
#elif VECT4
    constexpr uint SIMD = 4;
#elif VECT8
    constexpr uint SIMD = 8;
#endif

#if SUBGROUP_32
    constexpr uint subgroupSize = 32;
#elif SUBGROUP_16 
    constexpr uint subgroupSize = 16;
#elif SUBGROUP_8
    constexpr int soubgroupSize = 8;
#endif

#if SHARED_BANKS_32
    constexpr int extraCols = SIMD + 1;
#elif SHARED_BANKS_16
    constexpr int extracCols = 2 * SIMD + 1
#endif


constexpr uint BM = 64;
constexpr uint BN = 64;
constexpr uint BK = 8;
constexpr uint TM = 8;
constexpr uint TN = 8;

namespace {
    __kernel void LoadFromGlobal(int N, int K, float *A, float *B, float *As, float *Bs, int innerRowA, int innerColA, int innerRowB, 
                                    int innerColB, int rowStrideA, int rowStrideB, event_t& event) {
        for(uint offset = 0; offset + rowStrideA <= BM; offset += rowStrideA) {
#if VECT2
            async_work_group_copy(&As[(innerColA * SIMD + 0) * BM + innerRowA + offset],
                                    &A[(innerRowA + offset) * K + innerColA * SIMD], sizeof(float), event);
            async_work_group_copy(&As[(innerColA * SIMD + 1) * BM + innerRowA + offset],
                                    &A[(innerRowA + offset) * K + innerColA * SIMD + 1], sizeof(float), event);
#elif VECT4
            async_work_group_copy(&As[(innerColA * SIMD + 0) * BM + innerRowA + offset],
                                    &A[(innerRowA + offset) * K + innerColA * SIMD], sizeof(float), event);
            async_work_group_copy(&As[(innerColA * SIMD + 1) * BM + innerRowA + offset],
                                    &A[(innerRowA + offset) * K + innerColA * SIMD + 1], sizeof(float), event);
            async_work_group_copy(&As[(innerColA * SIMD + 2) * BM + innerRowA + offset],
                                    &A[(innerRowA + offset) * K + innerColA * SIMD + 2], sizeof(float), event);
            async_work_group_copy(&As[(innerColA * SIMD + 3) * BM + innerRowA + offset],
                                    &A[(innerRowA + offset) * K + innerColA * SIMD + 3], sizeof(float), event);
#elif VECT8
            async_work_group_copy(&As[(innerColA * SIMD + 0) * BM + innerRowA + offset],
                                    &A[(innerRowA + offset) * K + innerColA * SIMD], sizeof(float), event);
            async_work_group_copy(&As[(innerColA * SIMD + 1) * BM + innerRowA + offset],
                                    &A[(innerRowA + offset) * K + innerColA * SIMD + 1], sizeof(float), event);
            async_work_group_copy(&As[(innerColA * SIMD + 2) * BM + innerRowA + offset],
                                    &A[(innerRowA + offset) * K + innerColA * SIMD + 2], sizeof(float), event);
            async_work_group_copy(&As[(innerColA * SIMD + 3) * BM + innerRowA + offset],
                                    &A[(innerRowA + offset) * K + innerColA * SIMD + 3], sizeof(float), event);
            async_work_group_copy(&As[(innerColA * SIMD + 4) * BM + innerRowA + offset],
                                    &A[(innerRowA + offset) * K + innerColA * SIMD + 4], sizeof(float), event);
            async_work_group_copy(&As[(innerColA * SIMD + 5) * BM + innerRowA + offset],
                                    &A[(innerRowA + offset) * K + innerColA * SIMD + 5], sizeof(float), event);
            async_work_group_copy(&As[(innerColA * SIMD + 6) * BM + innerRowA + offset],
                                    &A[(innerRowA + offset) * K + innerColA * SIMD + 6], sizeof(float), event);
            async_work_group_copy(&As[(innerColA * SIMD + 7) * BM + innerRowA + offset],
                                    &A[(innerRowA + offset) * K + innerColA * SIMD + 7], sizeof(float), event);
#endif
        }

        for(uint offset = 0; offset + rowStrideB <= BK; offset += rowStrideB) {
#if VECT2
            async_work_group_copy(&Bs[(innerRowB + offset) * (BN + extraCols) + innerColB * SIMD],
                                    &B[(innerRowB + offset) * N + innerColB * SIMD], sizeof(float), event);
            async_work_group_copy(&Bs[(innerRowB + offset) * (BN + extraCols) + innerColB * SIMD + 1],
                                        &B[(innerRowB + offset) * N + innerColB * SIMD + 1], sizeof(float), event);
#elif VECT4
            async_work_group_copy(&Bs[(innerRowB + offset) * (BN + extraCols) + innerColB * SIMD],
                                    &B[(innerRowB + offset) * N + innerColB * SIMD], sizeof(float), event);
            async_work_group_copy(&Bs[(innerRowB + offset) * (BN + extraCols) + innerColB * SIMD + 1],
                                        &B[(innerRowB + offset) * N + innerColB * SIMD + 1], sizeof(float), event);
            async_work_group_copy(&Bs[(innerRowB + offset) * (BN + extraCols) + innerColB * SIMD + 2],
                                    &B[(innerRowB + offset) * N + innerColB * SIMD + 2], sizeof(float), event);
            async_work_group_copy(&Bs[(innerRowB + offset) * (BN + extraCols) + innerColB * SIMD + 3],
                                        &B[(innerRowB + offset) * N + innerColB * SIMD + 3], sizeof(float), event);
#elif VECT8
            async_work_group_copy(&Bs[(innerRowB + offset) * (BN + extraCols) + innerColB * SIMD],
                                    &B[(innerRowB + offset) * N + innerColB * SIMD], sizeof(float), event);
            async_work_group_copy(&Bs[(innerRowB + offset) * (BN + extraCols) + innerColB * SIMD + 1],
                                        &B[(innerRowB + offset) * N + innerColB * SIMD + 1], sizeof(float), event);
            async_work_group_copy(&Bs[(innerRowB + offset) * (BN + extraCols) + innerColB * SIMD + 2],
                                    &B[(innerRowB + offset) * N + innerColB * SIMD + 2], sizeof(float), event);
            async_work_group_copy(&Bs[(innerRowB + offset) * (BN + extraCols) + innerColB * SIMD + 3],
                                        &B[(innerRowB + offset) * N + innerColB * SIMD + 3], sizeof(float), event);
            async_work_group_copy(&Bs[(innerRowB + offset) * (BN + extracols) + innerColB * SIMD + 4],
                                    &B[(innerRowB + offset) * N + innerColB * SIMD + 4], sizeof(float), event);
            async_work_group_copy(&Bs[(innerRowB + offset) * (BN + extracols) + innerColB * SIMD + 5],
                                        &B[(innerRowB + offset) * N + innerColB * SIMD + 5], sizeof(float), event);
            async_work_group_copy(&Bs[(innerRowB + offset) * (BN + extracols) + innerColB * SIMD + 6],
                                    &B[(innerRowB + offset) * N + innerColB * SIMD + 6], sizeof(float), event);
            async_work_group_copy(&Bs[(innerRowB + offset) * (BN + extracols) + innerColB * SIMD + 7],
                                        &B[(innerRowB + offset) * N + innerColB * SIMD + 7], sizeof(float), event);
#endif
        }

    }

    __kernel void computeFromSmem(const uint SM, const uint SN, const int SMITER, const int SNITER, const int SSUBM, const int SSUBN, float *regM, float *regN, 
                                    float *threadResults, const float *As, const float *Bs, const uint subRowIdx, const uint subColIdx, const uint threadRowInSub, 
                                    const uint threadColInSub) {
        
        for(uint dotIdx = 0; dotIdx < BK; ++dotIdx) {
            for(uint sSubRowIdx = 0; sSubRowIdx < SMITER; ++sSubRowIdx) {
                for(uint i = 0; i < TM; ++i) {
                    regM[sSubRowIdx * TM + i] = As[(dotIdx * BM) + (subRowIdx * SM) + (sSubRowIdx * SSUBM) + threadRowInSub * TM + i];
                }
            }
            for(uint subColIdx = 0; subColIdx < SNITER; ++subColIdx) {
                for(uint i = 0; i < TN; ++i) {
                    regN[sSubColIdx * TN + i] = Bs[(dotIdx * BN) + (subColIdx * SN) + (sSubColIdx * SSUBN) + threadColInSub * TN + i];
                }
            }
            for(uint sSubRowIdx = 0; sSubRowIdx < SMITER; ++sSubRowIdx) {
                for(uint sSubColIdx = 0; sSubColIdx < SNITER; ++sSubColIdx) {
                    for(uint resIdxM = 0; resIdxM < TM; ++resIdxM) {
                        for(uint resIdxN = 0; resIdxN < TN; ++resIdxN) {
                            threadResults[(sSubRowIdx * TM + resIdxM) * (SNITER * TN) + sSubColIdx * TN + resIdxN] += regM[sSubRowIdx * TM +resIdxM] * 
                                                                                                                        regN[sSubColIdx * TN + resIdxN];
                        }
                    }
                }
            }
        }
    }
                        
                    
} //namespace

__kernel __attribute__((reqd_work_group_size(X, Y, Z))) 
         __attribute__((intel_reqd_sub_group_size(subgroupSize)))
void mat_mul(int M, int N, int K, __global const float *A, __global const float *B, 
                        __global float *C, __constant float* __restrict__ alpha) 
{
    
    const uint cRow = get_group_id(1);
    const uint cCol = get_group_id(0);
    const uint numThreads = get_global_size(0) * get_global_size(1) * get_global_size(2);
    const uint subgroupSize = get_sub_group_size();
    

    // position of subgroup in block tile
    const uint subgroupIdx = get_local_id(0) % subgroupSize;
    const uint subCol =  subgroupIdx % (BN / SN);
    const uint subRow = subgroupIdx / (BN / SN);

    constexpr uint SMITER = (SN * SM) / (subgroupSize * TM * TN * SNITER);
    constexpr uint SSUBM = SM / SMITER;
    constexpr uint SSUBN = SN/ SNITER;

    const uint threadIdxInSubgroup = get_local_id(0) % subgroupSize;
    const uint threadColInSubgroup = threadIdxInSubgroup % (SSUBN / TN);
    const uint threadRowInSubgroup = threadIdxSubgroup / (SSBUN / TN);


    const uint threadCol = get_local_id(0) % (SN / TN);
    const uint threadRow = get_local_id(0) / (SN / TN);
    const uint innerColA = get_local_id(0) % (BK / SIMD);
    const uint innerRowA = get_local_id(0) / (BK / SIMD);
    const uint innerColB = get_local_id(0) % (BN / SIMD);
    const uint innerRowB = get_local_id(0) / (BN / SIMD);

    const uint rowStrideA = (numThreads * SIMD) / BK ;
    const uint rowStrideB = numThreads / (BK / SIMD);

    __local event_t frontEvent;
    __local event_t backEvent;

    auto frontEventPtr = &frontEvent;
    auto beckEventPtr = &backEvent; 

    __local float As[2 * BM * BK];
    __local float Bs[2 * BK * (BN + extraCols)];
    
    __private float threadResults[SMITER * SNITER *TM * TN] = {0.0f};
    __private float regM[TM] = {0.0f};
    __private float regN[TN] = {0.0f};


    A += cRow * BM * K;                          //row=cRow, col=0
    B += cCol * BN;                              //row=0, col=cCol
    C += (cRow * BM + subRow * SM) * N + cCol * BN + subcCol * SN;

    //switch between double buffer chunks
    int As_offset = 0;
    int Bs_offset = 0;

    *frontEventPtr = LoadFromGmem(N, K, A, B, As + As_offset * BM * BK, Bs + Bs_offset * BK * BN, innerRowA, innerColA, innerRowB, innerColB, rowStrideA, rowStrideB, (event_t)0);    

    for(uint bkIdx = 0; bkIdx < K - BK; bkIdx += BK) {
        *backEventPtr = LoadFromGmem(N, K, A + BK, B + BK * N, As + (1 - As_offset) * BM * BK, Bs + (1 - Bs_offset) * BK * BN, innerRowA, innerColA, innerRowB, innerColB, 
                                    rowStrideA, rowStrideB, (event_t)0);
        wait_group_events(1, frontEventPtr);
        computeFromSmem(SM, SN, SMITER, SNITER, SSUBM, SSUBM, regM, regN, threadResults, As + As_offset * BM * BK, Bs + Bs_offset * BK * BN, subRow, subCol, 
                        threadRowInSubgroup, threadColInSubgroup);

        A += BK;
        B += BK * N;

    
        As_offset = 1 - As_offset;
        Bs_offset = 1 - Bs_offset;

        auto tmpEvent = frontEventPtr;
        frontEventPtr = backEventPtr;
        backEventPtr = tmpEvent;

        barrier(CLK_LOCAL_MEM_FENCE);
    }

    wait_group_events(1, frontEventPtr);
    computeFromSmem(SM, SN, SMITER, SNITER, SSUBM, SSUBM, regM, regN, threadResults, As + As_offset * BM * BK, Bs + Bs_offset * BK * BN, subRow, subCol, 
                    threadRowInSubgroup, threadColInSubgroup);


    for(uint sSubRowIdx = 0; sSubRowIdx < SMITER; ++sSubRowIdx) {
        for(uint sSubColIdx = 0; sSubColIdx < SNITER; ++sSubColIdx) {
            float *C_simd = C + (sSubRowIdx * SSUBM * N) + (sSubColIdx * SSUBN);
            for(uint resIdxM = 0; resIdxM < TM; ++resIdxM) {
                for(auto resIdxN = 0; resIdxN < TN; resIdxN += 4) {
#if VECT2
                    float2 tmp = (float2)(C_simd[(threadRowInSubgroup * TM + resIdxM) * N +threadColInSubgroup * TN + resIdxN + 0], 
                                        C_simd[(threadRowInSubgroup * TM + resIdxM) * N +threadColInSubgroup * TN + resIdxN + 1]);
#elif VECT4
                    float4 tmp = (float4)(C_simd[(threadRowInSubgroup * TM + resIdxM) * N +threadColInSubgroup * TN + resIdxN + 0], 
                                        C_simd[(threadRowInSubgroup * TM + resIdxM) * N +threadColInSubgroup * TN + resIdxN + 1],
                                        C_simd[(threadRowInSubgroup * TM + resIdxM) * N +threadColInSubgroup * TN + resIdxN + 2], 
                                        C_simd[(threadRowInSubgroup * TM + resIdxM) * N +threadColInSubgroup * TN + resIdxN + 3]);
#elif VECT8

                    float8 tmp = (float8)(C_simd[(threadRowInSubgroup * TM + resIdxM) * N +threadColSubgroup * TN + resIdxN + 0], 
                                        C_simd[(threadRowInSubgroup * TM + resIdxM) * N +threadColInSubgroup * TN + resIdxN + 1],
                                        C_simd[(threadRowInSubgroup * TM + resIdxM) * N +threadColInSubgroup * TN + resIdxN + 2], 
                                        C_simd[(threadRowInSubgroup * TM + resIdxM) * N +threadColInSubgroup * TN + resIdxN + 3],
                                        C_simd[(threadRowInSubgroup * TM + resIdxM) * N +threadColInSubgroup * TN + resIdxN + 4], 
                                        C_simd[(threadRowInSubgroup * TM + resIdxM) * N +threadColInSubgroup * TN + resIdxN + 5],
                                        C_simd[(threadRowInSubgroup * TM + resIdxM) * N +threadColInSubgroup * TN + resIdxN + 6], 
                                        C_simd[(threadRowInSubgroup * TM + resIdxM) * N +threadColInSubgroup * TN + resIdxN + 7]);
#endif
                    const int i = (sSubRowIdx * TM + resIdxM) * (SNITER * TN) + sSubColIdx * TN + resIdxN;
#if VECT2
                    tmp.s0 = *alpha * threadResults[i + 0];
                    tmp.s1 = *alpha * threadResults[i + 1];
#elif VECT4
                    tmp.s0 = *alpha * threadResults[i + 0];
                    tmp.s1 = *alpha * threadResults[i + 1];
                    tmp.s2 = *alpha * threadResults[i + 2];
                    tmp.s3 = *alpha * threadResults[i + 3];
#elif VECT8
                    tmp.s0 = *alpha * threadResults[i + 0];
                    tmp.s1 = *alpha * threadResults[i + 1];
                    tmp.s2 = *alpha * threadResults[i + 2];
                    tmp.s3 = *alpha * threadResults[i + 3];
                    tmp.s4 = *alpha * threadResults[i + 4];
                    tmp.s5 = *alpha * threadResults[i + 5];
                    tmp.s6 = *alpha * threadResults[i + 6];
                    tmp.s7 = *alpha * threadResults[i + 7];
#endif

#if VECT2

                    C_simd[(threadRowInSubgroup * TM + resIdxM) * N + threadColInSubgroup * TN + resIdxN + 0] = tmp.s0;
                    C_simd[(threadRowInSubgroup * TM + resIdxM) * N + threadColInSubgroup * TN + resIdxN + 1] = tmp.s1;
#elif VECT4
                    C_simd[(threadRowInSubgroup * TM + resIdxM) * N + threadColInSubgroup * TN + resIdxN + 0] = tmp.s0;
                    C_simd[(threadRowInSubgroup * TM + resIdxM) * N + threadColInSubgroup * TN + resIdxN + 1] = tmp.s1;
                    C_simd[(threadRowInSubgroup * TM + resIdxM) * N + threadColInSubgroup * TN + resIdxN + 2] = tmp.s2;
                    C_simd[(threadRowInSubgroup * TM + resIdxM) * N + threadColInSubgroup * TN + resIdxN + 3] = tmp.s3;
#elif VECT8
                    C_simd[(threadRowInSubgroup * TM + resIdxM) * N + threadColInSubgroup * TN + resIdxN + 0] = tmp.s0;
                    C_simd[(threadRowInSubgroup * TM + resIdxM) * N + threadColInSubgroup * TN + resIdxN + 1] = tmp.s1;
                    C_simd[(threadRowInSubgroup * TM + resIdxM) * N + threadColInSubgroup * TN + resIdxN + 2] = tmp.s2;
                    C_simd[(threadRowInSubgroup * TM + resIdxM) * N + threadColInSubgroup * TN + resIdxN + 3] = tmp.s3;
                    C_simd[(threadRowInSubgroup * TM + resIdxM) * N + threadColInSubgroup * TN + resIdxN + 4] = tmp.s4;
                    C_simd[(threadRowInSubgroup * TM + resIdxM) * N + threadColInSubgroup * TN + resIdxN + 5] = tmp.s5;
                    C_simd[(threadRowInSubgroup * TM + resIdxM) * N + threadColInSubgroup * TN + resIdxN + 6] = tmp.s6;
                    C_simd[(threadRowInSubgroup * TM + resIdxM) * N + threadColInSubgroup * TN + resIdxN + 7] = tmp.s7;
#endif
                }
            }
        
        } 
    }
    
}
